{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef03763d-ddc7-4048-ab64-cbbead80d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88725495-dd3c-410e-b319-65d5d26cc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN NAMES\n",
    "\n",
    "columns = [\"projectID\", \"commitID\", \"blobId\" , \"url\", \"readme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6af5652-7c5b-4520-9f4b-056065415a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#ProjectID;LatestCommitDate;EarliestCommitDate;NumCommits;NumForks;FileInfo;NumCore;NumAuthors;NumBlobs;Core;NumFiles;Gender;NumActiveMon;Communit\n",
    "#ySize;NumStars;RootFork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c5af2e-2155-49e6-843e-ea9e4db0ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ProjectID', 'LatestCommitDate', 'EarliestCommitDate', 'NumCommits',\n",
      "       'NumForks', 'FileInfo', 'NumCore', 'NumAuthors', 'NumBlobs', 'Core',\n",
      "       'NumFiles', 'Gender', 'NumActiveMon', 'CommunitySize', 'NumStars',\n",
      "       'RootFork'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "metadatafile_path ='./take2/unzipping/input_project_list_full_attributes.csv'\n",
    "dfmetadata = pd.read_csv(metadatafile_path,sep=';')\n",
    "column_names = dfmetadata.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd83565b-0da5-4ff7-912b-93005a070990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1...\n",
      ">> Done processing file 1...\n",
      "Processing file 2...\n",
      ">> Done processing file 2...\n",
      "Processing file 3...\n",
      ">> Done processing file 3...\n",
      "Processing file 4...\n",
      ">> Done processing file 4...\n",
      "Processing file 5...\n",
      ">> Done processing file 5...\n",
      "Processing file 6...\n",
      ">> Done processing file 6...\n",
      "Processing file 7...\n",
      ">> Done processing file 7...\n",
      "Processing file 8...\n",
      ">> Done processing file 8...\n",
      "Processing file 9...\n",
      ">> Done processing file 9...\n",
      "Processing file 10...\n",
      ">> Done processing file 10...\n",
      "Processing file 11...\n",
      ">> Done processing file 11...\n",
      "Processing file 12...\n",
      ">> Done processing file 12...\n",
      "Processing file 13...\n",
      ">> Done processing file 13...\n",
      "Processing file 14...\n",
      ">> Done processing file 14...\n",
      "Processing file 15...\n",
      ">> Done processing file 15...\n",
      "Processing file 16...\n",
      ">> Done processing file 16...\n",
      "Processing file 17...\n",
      ">> Done processing file 17...\n",
      "Processing file 18...\n",
      ">> Done processing file 18...\n",
      "Processing file 19...\n",
      ">> Done processing file 19...\n",
      "Processing file 20...\n",
      ">> Done processing file 20...\n",
      "Processing file 21...\n",
      ">> Done processing file 21...\n",
      "Processing file 22...\n",
      ">> Done processing file 22...\n",
      "Processing file 23...\n",
      ">> Done processing file 23...\n",
      "Processing file 24...\n",
      ">> Done processing file 24...\n",
      "Processing file 25...\n",
      ">> Done processing file 25...\n",
      "Processing file 26...\n",
      ">> Done processing file 26...\n",
      "Processing file 27...\n",
      ">> Done processing file 27...\n",
      "Processing file 28...\n",
      ">> Done processing file 28...\n",
      "Processing file 29...\n",
      ">> Done processing file 29...\n",
      "Processing file 30...\n",
      ">> Done processing file 30...\n",
      "Processing file 31...\n",
      ">> Done processing file 31...\n",
      "Processing file 32...\n",
      ">> Done processing file 32...\n",
      "Processing file 33...\n",
      ">> Done processing file 33...\n",
      "Processing file 34...\n",
      ">> Done processing file 34...\n",
      "Processing file 35...\n",
      ">> Done processing file 35...\n",
      "Processing file 36...\n",
      ">> Done processing file 36...\n",
      "Processing file 37...\n",
      ">> Done processing file 37...\n",
      "Processing file 38...\n",
      ">> Done processing file 38...\n",
      "Processing file 39...\n",
      ">> Done processing file 39...\n",
      "Processing file 40...\n",
      ">> Done processing file 40...\n",
      "Processing file 41...\n",
      ">> Done processing file 41...\n",
      "Processing file 42...\n",
      ">> Done processing file 42...\n",
      "Processing file 43...\n",
      ">> Done processing file 43...\n",
      "Processing file 44...\n",
      ">> Done processing file 44...\n",
      "Processing file 45...\n",
      ">> Done processing file 45...\n",
      "Processing file 46...\n",
      ">> Done processing file 46...\n",
      "Processing file 47...\n",
      ">> Done processing file 47...\n",
      "Processing file 48...\n",
      ">> Done processing file 48...\n",
      "Processing file 49...\n",
      ">> Done processing file 49...\n",
      "Processing file 50...\n",
      ">> Done processing file 50...\n"
     ]
    }
   ],
   "source": [
    "metadatafile_path ='./take2/unzipping/input_project_list_full_attributes.csv'\n",
    "dfmetadata = pd.read_csv(metadatafile_path,sep=';')\n",
    "# Iterate from file one through fifty\n",
    "for i in range(1,51):\n",
    "    allprojects = {}\n",
    "    print(f\"Processing file {i}...\")\n",
    "    if(i<10):\n",
    "        file_path_prefix = './take2/unzipping/t3read_final_no_slash_readmemd_only_unique_rows_only_unique_project_names_0'\n",
    "\n",
    "    else:\n",
    "        file_path_prefix = './take2/unzipping/t3read_final_no_slash_readmemd_only_unique_rows_only_unique_project_names_'\n",
    "\n",
    "    file_path = file_path_prefix+str(i)\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            bcp = l.split(';')\n",
    "            if (len(bcp) >= 4):\n",
    "                project = {}\n",
    "                project['ProjectID'] = bcp[1]\n",
    "                #project['ProjectID'] = bcp[1].strip()\n",
    "                project['blobId'] = bcp[3]\n",
    "                project['commitID'] = bcp[0]\n",
    "\n",
    "                default_url_prefix = 'https://github.com/'\n",
    "                gitlab_url_prefix = 'https://'\n",
    "                url_prefix = default_url_prefix\n",
    "                urli = ''\n",
    "                if (project['ProjectID'].startswith('gitlab.com') or project['ProjectID'].startswith('bitbucket.org') or project['ProjectID'].startswith('git.launchpad.net')):\n",
    "                    url_prefix = gitlab_url_prefix\n",
    "                    urli = url_prefix + project['ProjectID'].replace('_', '/')\n",
    "                elif (project['ProjectID'].startswith('sourceforge.net')):\n",
    "                    url_prefix = 'https://sourceforge.net/projects'\n",
    "                    projectsuffix = project['ProjectID'].replace('sourceforge.net', '')\n",
    "                    urli = url_prefix + projectsuffix.replace('_', '/')\n",
    "                else:\n",
    "                    url_prefix = default_url_prefix\n",
    "                    urli = url_prefix + project['ProjectID'].replace('_', '/')\n",
    "\n",
    "                project['url'] = urli\n",
    "       \n",
    "                # Read the readme file\n",
    "                loc = file_path_prefix+str(i)+'-dir/'\n",
    "                filename = project['ProjectID']\n",
    "                try:\n",
    "                    with open(loc + filename, 'r', encoding='utf-8', errors='replace') as ff:\n",
    "                        llines = ff.readlines()\n",
    "                        project['readme'] = llines\n",
    "                        allprojects[bcp[1].strip()] = project\n",
    "                except FileNotFoundError:\n",
    "                    print('ERROR: file not found for project ' + project['ProjectID'] + \" Filename: '\" + loc + filename + \"'\")\n",
    "                    llines = \"file not found\"\n",
    "                    project['readme'] = llines\n",
    "                    allprojects[bcp[1].strip()] = project\n",
    "                    continue\n",
    "\n",
    "        # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(allprojects.values())\n",
    "\n",
    "    result_df = pd.merge(df, dfmetadata, on='ProjectID', how='inner')\n",
    "    #column_names = dfmetadata.columns\n",
    "\n",
    "        # Save the DataFrame to pickle and CSV files\n",
    "    result_df.to_pickle(f'./take2/unzipping/processed_data/processeddata{i}.pkl')\n",
    "    result_df.to_csv(f'./take2/unzipping/processed_data/processeddata{i}.csv')\n",
    "    print(f\">> Done processing file {i}...\")\n",
    "\n",
    "\n",
    "                # #assign metadata attributes\n",
    "                \n",
    "                # matching_rows = dfmetadata[dfmetadata['ProjectID'] == project['ProjectID']]\n",
    "\n",
    "                # column_names = ['ProjectID', 'LatestCommitDate', 'EarliestCommitDate', 'NumCommits',\n",
    "                # 'NumForks', 'FileInfo', 'NumCore', 'NumAuthors', 'NumBlobs', 'Core',\n",
    "                # 'NumFiles', 'Gender', 'NumActiveMon', 'CommunitySize', 'NumStars', 'RootFork']\n",
    "\n",
    "                # for column in column_names:\n",
    "                #     project[column] = matching_rows[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44a7f937-d853-450d-8f06-ab91661a1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/processeddata1.pkl\", \"rb\") as f:\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91528ee6-d1be-4fae-8c3e-b665ada27d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/processeddata1.pkl\", \"rb\") as f:\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "dfverify.sample(2)\n",
    "dfverify.head(50)\n",
    "dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41f018-6dfd-4e6d-8794-07df25c55f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out llm - del later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350ecfc-dd8b-4625-bd9b-aec594147a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/finalscidata-1-1.pkl\", \"rb\") as f:\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "dfverify.sample(2)\n",
    "dfverify.head(50)\n",
    "dfverify.tail(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
