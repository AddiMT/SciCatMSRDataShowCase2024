{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef03763d-ddc7-4048-ab64-cbbead80d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88725495-dd3c-410e-b319-65d5d26cc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN NAMES\n",
    "\n",
    "columns = [\"projectID\", \"commitID\", \"blobId\" , \"url\", \"readme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af5652-7c5b-4520-9f4b-056065415a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#ProjectID;LatestCommitDate;EarliestCommitDate;NumCommits;NumForks;FileInfo;NumCore;NumAuthors;NumBlobs;Core;NumFiles;Gender;NumActiveMon;Communit\n",
    "#ySize;NumStars;RootFork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7c5af2e-2155-49e6-843e-ea9e4db0ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ProjectID', 'LatestCommitDate', 'EarliestCommitDate', 'NumCommits',\n",
      "       'NumForks', 'FileInfo', 'NumCore', 'NumAuthors', 'NumBlobs', 'Core',\n",
      "       'NumFiles', 'Gender', 'NumActiveMon', 'CommunitySize', 'NumStars',\n",
      "       'RootFork'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "metadatafile_path ='./take2/unzipping/input_project_list_full_attributes.csv'\n",
    "dfmetadata = pd.read_csv(metadatafile_path,sep=';')\n",
    "column_names = dfmetadata.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd83565b-0da5-4ff7-912b-93005a070990",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatafile_path ='./take2/unzipping/input_project_list_full_attributes.csv'\n",
    "dfmetadata = pd.read_csv(metadatafile_path,sep=';')\n",
    "# Iterate from file one through fifty\n",
    "for i in range(1,2):\n",
    "    allprojects = {}\n",
    "    if(i<10):\n",
    "        file_path_prefix = './take2/unzipping/t3read_final_no_slash_readmemd_only_unique_rows_only_unique_project_names_0'\n",
    "\n",
    "    else:\n",
    "        file_path_prefix = './take2/unzipping/t3read_final_no_slash_readmemd_only_unique_rows_only_unique_project_names_'\n",
    "\n",
    "    file_path = file_path_prefix+str(i)\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            bcp = l.split(';')\n",
    "            if (len(bcp) >= 4):\n",
    "                project = {}\n",
    "                project['ProjectID'] = bcp[1]\n",
    "                #project['ProjectID'] = bcp[1].strip()\n",
    "                project['blobId'] = bcp[3]\n",
    "                project['commitID'] = bcp[0]\n",
    "\n",
    "                default_url_prefix = 'https://github.com/'\n",
    "                gitlab_url_prefix = 'https://'\n",
    "                url_prefix = default_url_prefix\n",
    "                urli = ''\n",
    "                if (project['ProjectID'].startswith('gitlab.com') or project['ProjectID'].startswith('bitbucket.org') or project['ProjectID'].startswith('git.launchpad.net')):\n",
    "                    url_prefix = gitlab_url_prefix\n",
    "                    urli = url_prefix + project['ProjectID'].replace('_', '/')\n",
    "                elif (project['ProjectID'].startswith('sourceforge.net')):\n",
    "                    url_prefix = 'https://sourceforge.net/projects'\n",
    "                    projectsuffix = project['ProjectID'].replace('sourceforge.net', '')\n",
    "                    urli = url_prefix + projectsuffix.replace('_', '/')\n",
    "                else:\n",
    "                    url_prefix = default_url_prefix\n",
    "                    urli = url_prefix + project['ProjectID'].replace('_', '/')\n",
    "\n",
    "                project['url'] = urli\n",
    "       \n",
    "                # Read the readme file\n",
    "                loc = file_path_prefix+str(i)+'-dir/'\n",
    "                filename = project['ProjectID']\n",
    "                try:\n",
    "                    with open(loc + filename, 'r', encoding='utf-8', errors='replace') as ff:\n",
    "                        llines = ff.readlines()\n",
    "                        project['readme'] = llines\n",
    "                        allprojects[bcp[1].strip()] = project\n",
    "                except FileNotFoundError:\n",
    "                    print('ERROR: file not found for project ' + project['ProjectID'] + \" Filename: '\" + loc + filename + \"'\")\n",
    "                    llines = \"file not found\"\n",
    "                    project['readme'] = llines\n",
    "                    allprojects[bcp[1].strip()] = project\n",
    "                    continue\n",
    "\n",
    "        # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(allprojects.values())\n",
    "\n",
    "    result_df = pd.merge(df, dfmetadata, on='ProjectID', how='inner')\n",
    "    #column_names = dfmetadata.columns\n",
    "\n",
    "        # Save the DataFrame to pickle and CSV files\n",
    "    result_df.to_pickle(f'./take2/unzipping/processed_data/processeddata{i}.pkl')\n",
    "    result_df.to_csv(f'./take2/unzipping/processed_data/processeddata{i}.csv')\n",
    "\n",
    "\n",
    "                # #assign metadata attributes\n",
    "                \n",
    "                # matching_rows = dfmetadata[dfmetadata['ProjectID'] == project['ProjectID']]\n",
    "\n",
    "                # column_names = ['ProjectID', 'LatestCommitDate', 'EarliestCommitDate', 'NumCommits',\n",
    "                # 'NumForks', 'FileInfo', 'NumCore', 'NumAuthors', 'NumBlobs', 'Core',\n",
    "                # 'NumFiles', 'Gender', 'NumActiveMon', 'CommunitySize', 'NumStars', 'RootFork']\n",
    "\n",
    "                # for column in column_names:\n",
    "                #     project[column] = matching_rows[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a7f937-d853-450d-8f06-ab91661a1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/finalscidata-39-28.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "#dfverify.sample(2)\n",
    "dfverify.head(50)\n",
    "dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91528ee6-d1be-4fae-8c3e-b665ada27d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfverify.shape)\n",
    "#dfverify.sample(2)\n",
    "dfverify.head(50)\n",
    "dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41f018-6dfd-4e6d-8794-07df25c55f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#try data combinne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89ae55-db34-4910-b38d-e74018de6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da13d19-e5e3-4ed8-9077-ba9bd239a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# Path where the .pkl files are stored\n",
    "path = './take2/unzipping/processed_data/scidata/'\n",
    "\n",
    "# Find all .pkl files matching the pattern 'finalscidata-39-*.pkl'\n",
    "file_pattern = path + 'finalscidata-39-*.pkl'\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Load each .pkl file and append its DataFrame to the list\n",
    "for file in file_list:\n",
    "    with open(file, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Optional: limit the number of rows if necessary\n",
    "# combined_df = combined_df.iloc[:num]\n",
    "\n",
    "# Save the combined DataFrame to a new .pkl file\n",
    "output_file = path + 'combined-finalscidata-test-39.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(combined_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c35c45-9365-42e0-9d9d-24849eebad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/combined-finalscidata-test-39.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "#dfverify.sample(2)\n",
    "dfverify.head(50)\n",
    "dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6da570-89c0-41ac-8d14-91b023570010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop for all files = readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2609b15d-2ea5-4ed6-b930-4516c7d7b41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342656, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# Path where the .pkl files are stored\n",
    "path = './take2/unzipping/processed_data/scidata/'\n",
    "processPath = './take2/unzipping/processed_data/'\n",
    "\n",
    "# Initialize an empty list to store all DataFrames from all iterations\n",
    "all_dataframes = []\n",
    "all_dataframesp = []\n",
    "all_dataframesrdme = []\n",
    "# Loop over the range 1 through 50\n",
    "for i in range(1, 51):\n",
    "    # Generate the file pattern for the current number\n",
    "    file_pattern = path + f'finalscidata-{i}-*.pkl'\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    pro_file_pattern = processPath +f'processeddata{i}.pkl'\n",
    "    pro_file_list = glob.glob(pro_file_pattern)\n",
    "\n",
    "    # Initialize an empty list to store DataFrames for the current iteration\n",
    "    dataframes = []\n",
    "    dataframespro = []\n",
    "    dataframesrdme = []\n",
    "\n",
    "    # Load each .pkl file and append its DataFrame to the list\n",
    "    for file in file_list:\n",
    "        with open(file, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "            dataframes.append(df)\n",
    "\n",
    "        # Load each pro.pkl file and append its DataFrame to the list\n",
    "    for profile in pro_file_list:\n",
    "        with open(profile, 'rb') as fp:\n",
    "            dfp = pickle.load(fp)\n",
    "            dataframespro.append(dfp)\n",
    "\n",
    "\n",
    "    # Concatenate all DataFrames from the current iteration into one\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        all_dataframes.append(combined_df)\n",
    "\n",
    "    # Concatenate all DataFrames from the current iteration into one\n",
    "    if dataframespro:\n",
    "        combined_dfp = pd.concat(dataframespro, ignore_index=True)\n",
    "        all_dataframesp.append(combined_dfp)\n",
    "\n",
    "\n",
    "# Concatenate all DataFrames from all iterations into one final DataFrame\n",
    "final_combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "final_combined_df = final_combined_df.drop_duplicates(subset='ProjectID')\n",
    "\n",
    "# Save the final combined DataFrame to a new .pkl file\n",
    "output_file = path + 'final_combined_scidata-all-final-v-newtest.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(final_combined_df, f)\n",
    "\n",
    "# Concatenate all DataFrames from all iterations into one final DataFrame\n",
    "final_combined_dfp = pd.concat(all_dataframesp, ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "final_combined_dfp = final_combined_dfp.drop_duplicates(subset='ProjectID')\n",
    "\n",
    "# Save the final combined DataFrame to a new .pkl file\n",
    "output_filep = path + 'final_combined_processeddata-all-final-v-newtest.pkl'\n",
    "with open(output_filep, 'wb') as f:\n",
    "    pickle.dump(final_combined_dfp, f)\n",
    "\n",
    "\n",
    "#final_combined_df_wthreadme = pfinal_combined_df.merge(final_combined_dfp, )\n",
    "final_combined_df_wthreadme = pd.merge(final_combined_df, final_combined_dfp) #, on='ProjectID', how='inner')\n",
    "\n",
    "print(final_combined_df_wthreadme.shape)\n",
    "final_combined_df_wthreadme.sample(2)\n",
    "final_combined_df_wthreadme.head(50)\n",
    "final_combined_df_wthreadme.sample(2)\n",
    "\n",
    "# Save the final combined DataFrame to a new .pkl file\n",
    "output_file_wthreadme = path + 'final_combined_scidatawreadme-all-final-v2.pkl'\n",
    "with open(output_file_wthreadme, 'wb') as fpr:\n",
    "    pickle.dump(final_combined_df_wthreadme, fpr)\n",
    "\n",
    "# final_combined_df_wthreadme = pd.merge(final_combined_df, final_combined_df_wthreadme)\n",
    "# # Save the final combined DataFrame to a new .pkl file\n",
    "# output_file_readme = path + 'final_combined_scidatawreadme-all-final-v-newtest.pkl'\n",
    "# with open(output_file_readme, 'wb') as fpr:\n",
    "#     pickle.dump(final_combined_df_wthreadme, fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda940c-9989-48ae-ba43-db36e372a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_scidatawreadme-all-final-v2.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "dfverify.to_csv(\"./take2/unzipping/processed_data/scidata/final_combined_scidatawreadme-all-final-v2.csv\", index=False) \n",
    "\n",
    "print(dfverify.shape)\n",
    "print(dfverify.columns)\n",
    "dfverify.sample(2)\n",
    "dfverify.head(3)\n",
    "#dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ae6c8-1add-41a5-b2d0-6b06fb260f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_scidata-all-final-v-newtest.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "print(dfverify.columns)\n",
    "dfverify.sample(2)\n",
    "\n",
    "dfverify.head(3)\n",
    "#dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd9b12f-f8a8-40db-ad50-b3fdb6cad274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_combined_processeddata-all-final-v-newtest.pkl'\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_processeddata-all-final-v-newtest.pkl\", \"rb\") as fp:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverifyp = pickle.load(fp)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverifyp.shape)\n",
    "dfverifyp.sample(2)\n",
    "dfverifyp.head(3)\n",
    "#dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295dd2c-a711-478a-9013-389854e0d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/processeddata1.pkl\", \"rb\") as fp:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverifyp = pickle.load(fp)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverifyp.shape)\n",
    "dfverifyp.sample(2)\n",
    "dfverifyp.head(50)\n",
    "#dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa287600-fb75-4edc-a49a-bd7bd92d09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_scidata-all-final-v-newtest.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "dfverify.sample(2)\n",
    "dfverify.head(50)\n",
    "#dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c6435-ca16-412d-88ae-30577a1e0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223727ea-2193-42be-b20e-40e03b1ad266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# Path where the .pkl files are stored\n",
    "path = './take2/unzipping/processed_data/scidata/'\n",
    "\n",
    "# Initialize an empty list to store all DataFrames from all iterations\n",
    "all_dataframes = []\n",
    "\n",
    "# Loop over the range 1 through 50\n",
    "for i in range(1, 51):\n",
    "    # Generate the file pattern for the current number\n",
    "    file_pattern = path + f'finalscidata-{i}-*.pkl'\n",
    "    file_list = glob.glob(file_pattern)\n",
    "\n",
    "    # Initialize an empty list to store DataFrames for the current iteration\n",
    "    dataframes = []\n",
    "\n",
    "    # Load each .pkl file and append its DataFrame to the list\n",
    "    for file in file_list:\n",
    "        with open(file, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "            dataframes.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames from the current iteration into one\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        all_dataframes.append(combined_df)\n",
    "\n",
    "# Concatenate all DataFrames from all iterations into one final DataFrame\n",
    "final_combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "final_combined_df = final_combined_df.drop_duplicates(subset='ProjectID')\n",
    "\n",
    "# Save the final combined DataFrame to a new .pkl file\n",
    "output_file = path + 'final_combined_scidata-all-final-v1.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(final_combined_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84a3e72-a05f-49b1-a7e1-44544338eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82924, 24)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_scidata.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "#dfverify.sample(2)\n",
    "#dfverify.head(50)\n",
    "#dfverify.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee2f908c-3146-4de3-9014-9db44a7da5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69939, 24)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_scidata-v2.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "#dfverify.sample(2)\n",
    "#dfverify.head(50)\n",
    "#dfverify.tail(50)\n",
    "final_combined_scidata-all-final-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aeb1db-2b3f-4a51-bd6f-2646a2d12b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_scidata-all-final-v1.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "\n",
    "print(dfverify.shape)\n",
    "dfverify.sample(2)\n",
    "dfverify.head(50)\n",
    "#dfverify.tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007eeb11-dbeb-4363-b1eb-6d2c2a59e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with the path to your .pkl file\n",
    "path_to_pkl_file = \"./take2/unzipping/processed_data/scidata/final_combined_scidata-all-final-v1.pkl\"\n",
    "\n",
    "# Replace with the desired output path for your .csv file\n",
    "path_to_csv_file = \"./take2/unzipping/processed_data/scidata/final_combined_scidata-all-final-v1.csv\"\n",
    "\n",
    "# Read the .pkl file\n",
    "df = pd.read_pickle(path_to_pkl_file)\n",
    "\n",
    "# Convert the DataFrame to a CSV file\n",
    "df.to_csv(path_to_csv_file, index=False)\n",
    "\n",
    "print(f\"File converted and saved as '{path_to_csv_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bd051-dafd-4b1f-85bb-1675f8d6f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search dataset for NSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508fb72-abf6-4820-b423-d214c35ac1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_scidatawreadme-all-final-v2.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    dfverify = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "print(dfverify.shape)\n",
    "dfverify.sample(2)\n",
    "dfverify.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b4ae87b-a903-4838-bf21-d82316c6d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342656, 30)\n",
      "Empty DataFrame\n",
      "Columns: [ProjectID, url, isScientificAppSoftware, mentionsPaperOrFunding, isResearchSoftware, isScienceSupportSoftware, keywords, short_description, full_answer, NumForks, NumStars, NumFiles, NumAuthors, NumCoreAuthors, AuthorsGender, NumCommits, NumBlobs, CodeLanguages, NumActiveMon, LatestCommitDate, EarliestCommitDate, CommunitySize, blobId, commitID, readme, FileInfo, NumCore, Core, Gender, RootFork]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 30 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProjectID</th>\n",
       "      <th>url</th>\n",
       "      <th>isScientificAppSoftware</th>\n",
       "      <th>mentionsPaperOrFunding</th>\n",
       "      <th>isResearchSoftware</th>\n",
       "      <th>isScienceSupportSoftware</th>\n",
       "      <th>keywords</th>\n",
       "      <th>short_description</th>\n",
       "      <th>full_answer</th>\n",
       "      <th>NumForks</th>\n",
       "      <th>...</th>\n",
       "      <th>EarliestCommitDate</th>\n",
       "      <th>CommunitySize</th>\n",
       "      <th>blobId</th>\n",
       "      <th>commitID</th>\n",
       "      <th>readme</th>\n",
       "      <th>FileInfo</th>\n",
       "      <th>NumCore</th>\n",
       "      <th>Core</th>\n",
       "      <th>Gender</th>\n",
       "      <th>RootFork</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ProjectID, url, isScientificAppSoftware, mentionsPaperOrFunding, isResearchSoftware, isScienceSupportSoftware, keywords, short_description, full_answer, NumForks, NumStars, NumFiles, NumAuthors, NumCoreAuthors, AuthorsGender, NumCommits, NumBlobs, CodeLanguages, NumActiveMon, LatestCommitDate, EarliestCommitDate, CommunitySize, blobId, commitID, readme, FileInfo, NumCore, Core, Gender, RootFork]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 30 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "num=100\n",
    "with open(\"./take2/unzipping/processed_data/scidata/final_combined_scidatawreadme-all-final-v2.pkl\", \"rb\") as f:\n",
    "    #dfverify = pd.read_csv.load(f)\n",
    "    df = pickle.load(f)\n",
    "    #dfverify = pickle.load(f)[1:num]\n",
    "print(df.shape)\n",
    "df.sample(2)\n",
    "df.head(50)\n",
    "#dfverify.\n",
    "#dfverify.head(10)\n",
    "#dfverify.tail(50)\n",
    "# Filter the DataFrame\n",
    "desired_string = 'hyperion'\n",
    "# Create a boolean mask to filter rows where 'Column2' contains the target substring\n",
    "#matching_rows = dfverify[str(dfverify['readme']).contains(desired_string, case=False, na=False)]\n",
    "matching_rows = df[df['readme'].str.contains(desired_string, case=False, na=False)]\n",
    "\n",
    "#filtered_df = dfverify[dfverify['readme'].str.contains(desired_string, na=False)]\n",
    "#print(filtered_df)\n",
    "print(matching_rows)\n",
    "##print(matching_rows.shape)\n",
    "#filtered_df.sample(2)\n",
    "matching_rows.head(10)\n",
    "# # Function to generate all 4-character substrings of A\n",
    "# def generate_substrings(input_string):\n",
    "#     substrings = []\n",
    "#     for i in range(len(input_string) - 3):\n",
    "#         substrings.append(input_string[i:i+4])\n",
    "#     return substrings\n",
    "\n",
    "# # Generate 4-character substrings of A\n",
    "# substrings_A = generate_substrings(dfverify[)\n",
    "\n",
    "# # Check if any of the 4-character substrings of A do not exist in the DataFrame column\n",
    "# result = df[~df['column_name'].str.contains('|'.join(substrings_A))]\n",
    "\n",
    "# # Print the rows where any of the 4-character substrings of A are not found in the column\n",
    "# print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79259e2-664f-446e-8562-88e23a9892fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "desired_string = 'NSF grant'\n",
    "filtered_df = df[df['column_name'].str.contains(desired_string)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
